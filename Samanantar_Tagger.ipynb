{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bOaOi8PvK8nK","outputId":"aacf74dc-adfc-4507-d361-b6aeabc8cb39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in /home/user/.local/lib/python3.10/site-packages (4.36.0)\n","Requirement already satisfied: filelock in /home/user/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.26.1)\n","Requirement already satisfied: numpy>=1.17 in /home/user/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /home/user/.local/lib/python3.10/site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/user/.local/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /home/user/.local/lib/python3.10/site-packages (from transformers) (2.30.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /home/user/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/user/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/.local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","fatal: destination path 'awesome-align' already exists and is not an empty directory.\n","/home/user/112101050_Utsav/btp/samanantar_align/awesome-align\n","Defaulting to user installation because normal site-packages is not writeable\n","Obtaining file:///home/user/112101050_Utsav/btp/samanantar_align/awesome-align\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.5.2 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (0.15.2)\n","Requirement already satisfied: torch>=1.2.0 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (4.66.2)\n","Requirement already satisfied: numpy in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (1.23.5)\n","Requirement already satisfied: boto3 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (1.28.9)\n","Requirement already satisfied: filelock in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (3.13.1)\n","Requirement already satisfied: requests in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (2.30.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/user/.local/lib/python3.10/site-packages (from tokenizers>=0.5.2->awesome-align==0.1.7) (0.26.1)\n","Requirement already satisfied: typing-extensions in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (4.12.2)\n","Requirement already satisfied: sympy in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (1.13.1)\n","Requirement already satisfied: networkx in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (3.1)\n","Requirement already satisfied: jinja2 in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (2.0.0)\n","Requirement already satisfied: cmake in /home/user/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.2.0->awesome-align==0.1.7) (3.26.3)\n","Requirement already satisfied: lit in /home/user/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.2.0->awesome-align==0.1.7) (16.0.5)\n","Requirement already satisfied: botocore<1.32.0,>=1.31.9 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (1.31.9)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/.local/lib/python3.10/site-packages (from requests->awesome-align==0.1.7) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->awesome-align==0.1.7) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/.local/lib/python3.10/site-packages (from requests->awesome-align==0.1.7) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->awesome-align==0.1.7) (2020.6.20)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/user/.local/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.9->boto3->awesome-align==0.1.7) (2.8.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/user/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome-align==0.1.7) (2023.5.0)\n","Requirement already satisfied: packaging>=20.9 in /home/user/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome-align==0.1.7) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome-align==0.1.7) (5.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/user/.local/lib/python3.10/site-packages (from jinja2->torch>=1.2.0->awesome-align==0.1.7) (2.1.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/.local/lib/python3.10/site-packages (from sympy->torch>=1.2.0->awesome-align==0.1.7) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.9->boto3->awesome-align==0.1.7) (1.16.0)\n","Installing collected packages: awesome-align\n","  Attempting uninstall: awesome-align\n","    Found existing installation: awesome-align 0.1.7\n","    Uninstalling awesome-align-0.1.7:\n","      Successfully uninstalled awesome-align-0.1.7\n","  Running setup.py develop for awesome-align\n","Successfully installed awesome-align\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["2025-03-16 03:28:50.824843: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-03-16 03:28:50.875043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-16 03:28:51.560968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["Using device: GPU (NVIDIA RTX A4000)\n"]},{"name":"stderr","output_type":"stream","text":["/home/user/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_ner_pipeline\u001b[39m(device: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pipeline(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdslim/bert-base-NER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[0;32m---> 70\u001b[0m ner_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ner_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# ----- VALIDATE AND EXTRACT DATASET ZIP -----\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_and_extract\u001b[39m(zip_path, extract_path):\n","Cell \u001b[0;32mIn[1], line 61\u001b[0m, in \u001b[0;36mcreate_ner_pipeline\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_ner_pipeline\u001b[39m(device: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdslim/bert-base-NER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdslim/bert-base-NER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3694\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3686\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3687\u001b[0m     (\n\u001b[1;32m   3688\u001b[0m         model,\n\u001b[1;32m   3689\u001b[0m         missing_keys,\n\u001b[1;32m   3690\u001b[0m         unexpected_keys,\n\u001b[1;32m   3691\u001b[0m         mismatched_keys,\n\u001b[1;32m   3692\u001b[0m         offload_index,\n\u001b[1;32m   3693\u001b[0m         error_msgs,\n\u001b[0;32m-> 3694\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3712\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3713\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4045\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4036\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   4037\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   4038\u001b[0m         state_dict,\n\u001b[1;32m   4039\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4043\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   4044\u001b[0m     )\n\u001b[0;32m-> 4045\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4046\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4048\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   4049\u001b[0m \n\u001b[1;32m   4050\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:606\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 604 (4 times)]\u001b[0m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:600\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    598\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1942\u001b[0m         \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1944\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1945\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1946\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(key, param\u001b[38;5;241m.\u001b[39msize(), input_param\u001b[38;5;241m.\u001b[39msize(), ex\u001b[38;5;241m.\u001b[39margs))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# The below code takes input of parallel sentences(samanantar dataset in this case), performs pre processing, filtering, alignment (bidirectional) and generates BIO format tagged NER dataset.\n","# Inputs, alignment algorithm and process, filtering are all modular and open to customization.\n","# Feel free to reach out (Utsav B21).\n","\n","# Install required packages and clone awesome-align\n","!pip install transformers\n","!git clone https://github.com/neulab/awesome-align.git\n","%cd awesome-align\n","!pip install -e .\n","\n","import os\n","import zipfile\n","import json\n","import random\n","import concurrent.futures\n","from pathlib import Path\n","from collections import defaultdict\n","from functools import reduce\n","import operator\n","\n","import torch\n","from tqdm import tqdm\n","from transformers import pipeline, AutoTokenizer\n","\n","# ----- PARAMETERS & PATHS -----\n","DATA_DIR = Path(\"/home/user/112101050_Utsav/btp/samanantar_align/samanantar_data\")\n","DATA_DIR.mkdir(exist_ok=True)\n","ZIP_FILE = DATA_DIR / \"samanantar.zip\"\n","EXTRACT_DIR = DATA_DIR / \"extracted\"\n","\n","# Languages to process\n","LANGUAGE_CODES = {\n","    # \"hindi\": \"hi\",\n","    \"assamese\": \"as\",\n","    \"gujarati\": \"gu\",\n","    # \"marathi\": \"mr\",\n","    \"punjabi\": \"pa\"\n","}\n","\n","# Filtering parameters\n","MIN_TOKENS = 1          # minimum number of tokens per sentence\n","MAX_TOKENS = 100        # maximum number of tokens per sentence\n","\n","# ----- DEVICE SETUP -----\n","device_name = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n","print(f\"Using device: {device_name} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n","device = 0 if torch.cuda.is_available() else -1\n","if device != -1:\n","    torch.cuda.set_device(device)\n","\n","# ----- INITIALIZE TOKENIZER -----\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","\n","# ----- INITIALIZE NER PIPELINE -----\n","def create_ner_pipeline(device: int = -1):\n","    return pipeline(\n","        \"ner\",\n","        model=\"dslim/bert-base-NER\",\n","        tokenizer=\"dslim/bert-base-NER\",\n","        device=device,\n","        batch_size=16,\n","        aggregation_strategy=\"none\",\n","        torch_dtype=torch.float16 if device != -1 else None\n","    )\n","ner_pipeline = create_ner_pipeline(device=device)\n","\n","\n","# ----- VALIDATE AND EXTRACT DATASET ZIP -----\n","def validate_and_extract(zip_path, extract_path):\n","    if not zip_path.exists():\n","        raise FileNotFoundError(f\"ZIP file not found at {zip_path}.\")\n","    try:\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            bad_file = zip_ref.testzip()\n","            if bad_file:\n","                raise ValueError(f\"Corrupted file in ZIP: {bad_file}\")\n","    except Exception as e:\n","        raise ValueError(f\"Invalid ZIP file: {e}\") from None\n","\n","    if not extract_path.exists():\n","        extract_path.mkdir(parents=True, exist_ok=True)\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            with concurrent.futures.ThreadPoolExecutor() as executor:\n","                for file in zip_ref.namelist():\n","                    executor.submit(zip_ref.extract, file, extract_path)\n","        print(\"Extraction complete.\")\n","\n","#validate_and_extract(ZIP_FILE, EXTRACT_DIR)\n","\n","# ----- READ SENTENCE FILES -----\n","def process_language_file(en_file, lang_file):\n","    with open(en_file, 'r', encoding='utf-8') as en_f, open(lang_file, 'r', encoding='utf-8') as lang_f:\n","        english_sentences = [line.strip() for line in en_f if line.strip()]\n","        language_sentences = [line.strip() for line in lang_f if line.strip()]\n","    assert len(english_sentences) == len(language_sentences), \"Mismatch in sentence counts.\"\n","    return english_sentences, language_sentences\n","\n","# ----- MERGE ENTITIES -----\n","def merge_entities(entities, sentence):\n","    \"\"\"\n","    Merges consecutive entities of the same type if they are adjacent or overlapping.\n","    \"\"\"\n","    merged = []\n","    for entity in sorted(entities, key=lambda x: x['start']):\n","        full_tag = entity['entity']\n","        ent_type = full_tag.split('-')[-1] if '-' in full_tag else full_tag\n","        start, end = entity['start'], entity['end']\n","        if merged and merged[-1]['ent_type'] == ent_type:\n","            prev_end = merged[-1]['end']\n","            # If there's no actual text between previous entity and this entity, merge them\n","            if not sentence[prev_end:start].strip():\n","                merged[-1]['end'] = max(merged[-1]['end'], end)\n","                continue\n","        merged.append({'start': start, 'end': end, 'ent_type': ent_type})\n","    return merged\n","\n","# ----- STEP 1: FILTER BY ENTITY (FIRST FILTER) -----\n","def filter_by_entity(english_sentences, language_sentences):\n","    \"\"\"\n","    Runs NER on all English sentences to decide which have named entities.\n","    Keep all sentences that contain >=1 entity.\n","    Keep only 1% of those that have no entities.\n","    Returns the filtered subset of (english_sentences, language_sentences).\n","    \"\"\"\n","    assert len(english_sentences) == len(language_sentences)\n","\n","    batch_size = 64\n","    n = len(english_sentences)\n","    has_entity = [False] * n\n","\n","    # Process NER in batches\n","    idx = 0\n","    for start_idx in tqdm(range(0, n, batch_size), desc=\"First Filter: Checking NER\"):\n","        batch = english_sentences[start_idx:start_idx+batch_size]\n","        ner_results = ner_pipeline(batch)\n","        for i, entities in enumerate(ner_results):\n","            merged = merge_entities(entities, batch[i])\n","            if merged:  # if there's at least one entity\n","                has_entity[start_idx + i] = True\n","\n","    entity_indices = [i for i, val in enumerate(has_entity) if val]\n","    non_entity_indices = [i for i, val in enumerate(has_entity) if not val]\n","\n","    # We keep all entity sentences\n","    # For non-entity sentences, keep only 1%\n","    random.shuffle(non_entity_indices)\n","    sample_size = max(1, int(0.01 * len(non_entity_indices))) if len(non_entity_indices) > 0 else 0\n","    sampled_non_entity_indices = set(non_entity_indices[:sample_size])\n","\n","    final_indices = entity_indices + list(sampled_non_entity_indices)\n","    final_indices.sort()\n","\n","    filtered_en = [english_sentences[i] for i in final_indices]\n","    filtered_lang = [language_sentences[i] for i in final_indices]\n","\n","    print(f\"Total sentences: {len(english_sentences)}\")\n","    print(f\"  With entity: {len(entity_indices)}\")\n","    print(f\"  Without entity (sampled 1%): {sample_size} of {len(non_entity_indices)}\")\n","    print(f\"  => after first filter: {len(filtered_en)}\")\n","\n","    return filtered_en, filtered_lang\n","\n","# ----- RUN ALIGNMENT & KEEP TOP 30% (SECOND FILTER) -----\n","def run_alignment(english_sentences, language_sentences):\n","    \"\"\"\n","    Runs awesome-align forward & backward. Computes alignment-based scores\n","    (normalized by the number of words in each pair). Sorts by these scores\n","    and keeps top 30%.\n","    \"\"\"\n","    batch_size = 256 if device != -1 else 128\n","    forward_input = \"\\n\".join([f\"{eng} ||| {lang}\" for eng, lang in zip(english_sentences, language_sentences)])\n","    forward_input_path = f\"{DATA_DIR}/forward_input.txt\"\n","    forward_output_path = f\"{DATA_DIR}/forward_output.txt\"\n","\n","    with open(forward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(forward_input)\n","\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" if device != -1 else \"-1\"\n","\n","    # Forward alignment\n","    !python3 run_align.py \\\n","      --model_name_or_path bert-base-multilingual-cased \\\n","      --data_file \"{forward_input_path}\" \\\n","      --output_file \"{forward_output_path}\" \\\n","      --output_prob_file \"{DATA_DIR}/forward_probabilities.txt\" \\\n","      --output_word_file \"{DATA_DIR}/forward_word_pairs.txt\" \\\n","      --extraction 'softmax' \\\n","      --batch_size {batch_size} \\\n","      --cache_dir \"{DATA_DIR}/cache\"\n","\n","    # Backward alignment\n","    backward_input = \"\\n\".join([f\"{lang} ||| {eng}\" for eng, lang in zip(english_sentences, language_sentences)])\n","    backward_input_path = f\"{DATA_DIR}/backward_input.txt\"\n","    backward_output_path = f\"{DATA_DIR}/backward_output.txt\"\n","\n","    with open(backward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(backward_input)\n","\n","    !python3 run_align.py \\\n","      --model_name_or_path bert-base-multilingual-cased \\\n","      --data_file \"{backward_input_path}\" \\\n","      --output_file \"{backward_output_path}\" \\\n","      --output_prob_file \"{DATA_DIR}/backward_probabilities.txt\" \\\n","      --output_word_file \"{DATA_DIR}/backward_word_pairs.txt\" \\\n","      --extraction 'softmax' \\\n","      --batch_size {batch_size} \\\n","      --cache_dir \"{DATA_DIR}/cache\"\n","\n","    def read_alignments(path, expected_length):\n","        alignments = []\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                pairs = set()\n","                if line.strip():\n","                    for pair in line.strip().split():\n","                        try:\n","                            e, l = map(int, pair.split('-'))\n","                            pairs.add((e, l))\n","                        except:\n","                            continue\n","                alignments.append(pairs)\n","        # Ensure length matches\n","        while len(alignments) < expected_length:\n","            alignments.append(set())\n","        return alignments[:expected_length]\n","\n","    forward_alignments = read_alignments(forward_output_path, len(english_sentences))\n","    backward_alignments = read_alignments(backward_output_path, len(english_sentences))\n","\n","    # Read probabilities, but use \"average\" normalized by total words to match the second filter requirement\n","    def read_probabilities(prob_file, eng_sents, lang_sents):\n","        scores = []\n","        with open(prob_file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                numbers = list(map(float, line.strip().split()))\n","                # Normalized by total words in both sentences (or just English, if desired)\n","                total_words = len(eng_sents[i].split()) + len(lang_sents[i].split())\n","                if total_words == 0:\n","                    score = 0\n","                else:\n","                    score = sum(numbers) / float(total_words)\n","                scores.append(score)\n","        # Ensure scores length matches\n","        while len(scores) < len(eng_sents):\n","            scores.append(0)\n","        return scores\n","\n","    forward_scores = read_probabilities(f\"{DATA_DIR}/forward_probabilities.txt\",\n","                                        english_sentences, language_sentences)\n","\n","    # Sort by forward scores, pick top 30%\n","    threshold_index = int(len(forward_scores) * 0.3)\n","    # Indices sorted by forward_scores in descending order\n","    sorted_indices = sorted(range(len(forward_scores)), key=lambda i: forward_scores[i], reverse=True)\n","    top_indices = sorted_indices[:threshold_index]\n","\n","    # Keep all top 30% sentences regardless of alignment intersection\n","    final_english = [english_sentences[i] for i in top_indices]\n","    final_lang = [language_sentences[i] for i in top_indices]\n","    final_alignments = [{\"alignment\": forward_alignments[i] & backward_alignments[i]} for i in top_indices]\n","\n","    print(f\"  => after second filter (top 30%): {len(final_english)}\")\n","\n","    return final_english, final_lang, final_alignments\n","\n","def get_word_spans(sentence):\n","    \"\"\"Extract words and their character spans from a sentence using whitespace tokenization.\"\"\"\n","    words = []\n","    start = 0\n","    for word in sentence.split():\n","        # Find the start index of the word in the sentence\n","        while start < len(sentence) and sentence[start] == ' ':\n","            start += 1\n","        end = start + len(word)\n","        words.append((word, (start, end)))\n","        start = end\n","    return words\n","\n","# ----- TOKENIZATION UTILITY -----\n","def tokenize_with_offsets(sentence, tokenizer):\n","    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n","    offsets = encoding[\"offset_mapping\"]\n","    return tokens, offsets\n","\n","# ----- TRANSFER TAGS (FINAL STEP) -----\n","def transfer_tags(english_sentences, language_sentences, alignments):\n","    curated_tagged_dataset = []\n","    sample_data = {}\n","\n","    ner_results = []\n","    batch_size = 64\n","    for batch_start in tqdm(range(0, len(english_sentences), batch_size),\n","                          desc=\"Final Tag Transfer: NER on English\"):\n","        batch = english_sentences[batch_start:batch_start+batch_size]\n","        ner_results.extend(ner_pipeline(batch))\n","\n","    for idx, (eng_sent, lang_sent, align_info) in tqdm(\n","            enumerate(zip(english_sentences, language_sentences, alignments)),\n","            total=len(english_sentences),\n","            desc=\"Transferring tags\"\n","        ):\n","        # English tokenization and tagging (unchanged)\n","        eng_tokens, eng_offsets = tokenize_with_offsets(eng_sent, bert_tokenizer)\n","        if not (MIN_TOKENS <= len(eng_tokens) <= MAX_TOKENS):\n","            continue\n","\n","        english_entities = merge_entities(ner_results[idx], eng_sent)\n","        bio_tags = ['O'] * len(eng_tokens)\n","        for entity in english_entities:\n","            ent_type = entity['ent_type']\n","            ent_start = entity['start']\n","            ent_end = entity['end']\n","            overlapping_indices = []\n","            for i, (tok_start, tok_end) in enumerate(eng_offsets):\n","                if max(tok_start, ent_start) < min(tok_end, ent_end):\n","                    overlapping_indices.append(i)\n","            if overlapping_indices:\n","                bio_tags[overlapping_indices[0]] = f\"B-{ent_type}\"\n","                for j in overlapping_indices[1:]:\n","                    bio_tags[j] = f\"I-{ent_type}\"\n","\n","        # Target language processing (modified)\n","        lang_subword_tokens, lang_subword_offsets = tokenize_with_offsets(lang_sent, bert_tokenizer)\n","        if len(lang_subword_tokens) == 0:\n","            continue\n","\n","        alignment_map = defaultdict(list)\n","        for e_idx, l_idx in align_info[\"alignment\"]:\n","            if e_idx < len(eng_tokens) and l_idx < len(lang_subword_tokens):\n","                alignment_map[l_idx].append(e_idx)\n","\n","        lang_subword_tags = ['O'] * len(lang_subword_tokens)\n","        for l_idx in range(len(lang_subword_tags)):\n","            for e_idx in alignment_map[l_idx]:\n","                eng_tag = bio_tags[e_idx]\n","                if eng_tag != 'O':\n","                    lang_subword_tags[l_idx] = eng_tag\n","                    break\n","\n","        # Group subwords into words\n","        word_spans = get_word_spans(lang_sent)\n","        word_level_tokens = []\n","        word_level_tags = []\n","\n","        for word, (s_start, s_end) in word_spans:\n","            subword_indices = []\n","            for idx, (tok_start, tok_end) in enumerate(lang_subword_offsets):\n","                if tok_start >= s_start and tok_end <= s_end:\n","                    subword_indices.append(idx)\n","\n","            tags_in_word = [lang_subword_tags[idx] for idx in subword_indices\n","                          if idx < len(lang_subword_tags)]\n","\n","            # Take first non-O tag (now keeps I- tags as-is)\n","            word_tag = next((tag for tag in tags_in_word if tag != 'O'), 'O')\n","            word_level_tokens.append(word)\n","            word_level_tags.append(word_tag)\n","\n","        # Enforce strict BIO constraints\n","        current_ent = None\n","        for i in range(len(word_level_tags)):\n","            tag = word_level_tags[i]\n","\n","            if tag == 'O':\n","                current_ent = None\n","                continue\n","\n","            parts = tag.split('-', 1)\n","            if len(parts) != 2:\n","                word_level_tags[i] = 'O'\n","                continue\n","\n","            prefix, ent_type = parts\n","\n","            # Handle invalid I- at start or after O\n","            if prefix == 'I':\n","                if current_ent != ent_type:\n","                    # Convert to B- if not continuation\n","                    word_level_tags[i] = f'B-{ent_type}'\n","                    current_ent = ent_type\n","                else:\n","                    # Valid continuation, keep I-\n","                    pass\n","            elif prefix == 'B':\n","                current_ent = ent_type\n","            else:\n","                word_level_tags[i] = 'O'\n","                current_ent = None\n","\n","        # Final validation pass\n","        prev_tag = None\n","        for i in range(len(word_level_tags)):\n","            tag = word_level_tags[i]\n","            if tag.startswith('I-'):\n","                if prev_tag in [None, 'O'] or not prev_tag.endswith(tag.split('-')[1]):\n","                    # Convert standalone I- to B-\n","                    word_level_tags[i] = tag.replace('I-', 'B-', 1)\n","            prev_tag = word_level_tags[i]\n","\n","        # Apply length filter\n","        if not (MIN_TOKENS <= len(word_level_tokens) <= MAX_TOKENS):\n","            continue\n","\n","        # Store sample data\n","        if not sample_data:\n","            sample_data = {\n","                'english': eng_sent,\n","                'target': lang_sent,\n","                'english_tokens': list(zip(eng_tokens, bio_tags)),\n","                'target_final': list(zip(word_level_tokens, word_level_tags))\n","            }\n","\n","        curated_tagged_dataset.append(list(zip(word_level_tokens, word_level_tags)))\n","\n","    return curated_tagged_dataset, sample_data\n","\n","# ----- PROCESS ALL LANGUAGES -----\n","def process_all_languages():\n","    for lang, lang_code in LANGUAGE_CODES.items():\n","        lang_dir = EXTRACT_DIR / \"v2\" / f\"en-{lang_code}\"\n","        en_file = lang_dir / \"train.en\"\n","        lang_file = lang_dir / f\"train.{lang_code}\"\n","        if not en_file.exists() or not lang_file.exists():\n","            print(f\"Skipping {lang} - files not found\")\n","            continue\n","\n","        print(f\"\\nProcessing {lang}...\")\n","\n","        # 1) Read all sentences\n","        english_sentences, language_sentences = process_language_file(en_file, lang_file)\n","\n","        # 2) First Filter: Keep all NE + 1% No NE\n","        filtered_english_1, filtered_lang_1 = filter_by_entity(english_sentences, language_sentences)\n","\n","        # 3) Second Filter (Alignment-based): top 30% after scoring\n","        final_english, final_lang, alignments = run_alignment(filtered_english_1, filtered_lang_1)\n","\n","        # 4) Transfer tags\n","        tagged_language_dataset, sample_data = transfer_tags(final_english, final_lang, alignments)\n","\n","        # Show a verification sample\n","        print(f\"\\n--- {lang.upper()} VERIFICATION SAMPLE ---\")\n","        print(\"English Sentence:\", sample_data.get('english', 'N/A'))\n","        print(\"English Tokens & Tags:\", sample_data.get('english_tokens', 'N/A'))\n","        print(\"Target Sentence:\", sample_data.get('target', 'N/A'))\n","        print(\"Target Final Tags:\", sample_data.get('target_final', 'N/A'))\n","        print(\"-\" * 50)\n","\n","        # 5) Save to JSON\n","        json_dataset = []\n","        for tagged_sentence in tagged_language_dataset:\n","            tokens = [token for token, tag in tagged_sentence]\n","            tags = [tag for token, tag in tagged_sentence]\n","            json_dataset.append({\"tokens\": tokens, \"ner_tags\": tags})\n","\n","        output_dir = DATA_DIR / \"new_bio_tagged_samanantar\"\n","        output_dir.mkdir(exist_ok=True)\n","        output_file = output_dir / f\"{lang}_ner.json\"\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            json.dump(json_dataset, f, ensure_ascii=False, indent=2)\n","        print(f\"Saved {len(json_dataset)} tagged sentences to {output_file}\")\n","\n","\n","process_all_languages()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyu6T0FTK8nL","outputId":"27061fbc-0bd7-4620-f69d-7f968ae956d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in /home/user/.local/lib/python3.10/site-packages (4.31.0)\n","Requirement already satisfied: filelock in /home/user/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.26.1)\n","Requirement already satisfied: numpy>=1.17 in /home/user/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /home/user/.local/lib/python3.10/site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/user/.local/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /home/user/.local/lib/python3.10/site-packages (from transformers) (2.30.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/user/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /home/user/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/user/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/.local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","fatal: destination path 'awesome-align' already exists and is not an empty directory.\n","/home/user/112101050_Utsav/btp/samanantar_align/awesome-align\n","Defaulting to user installation because normal site-packages is not writeable\n","Obtaining file:///home/user/112101050_Utsav/btp/samanantar_align/awesome-align\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.5.2 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (0.13.3)\n","Requirement already satisfied: torch>=1.2.0 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (4.66.2)\n","Requirement already satisfied: numpy in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (1.23.5)\n","Requirement already satisfied: boto3 in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (1.28.9)\n","Requirement already satisfied: filelock in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (3.13.1)\n","Requirement already satisfied: requests in /home/user/.local/lib/python3.10/site-packages (from awesome-align==0.1.7) (2.30.0)\n","Requirement already satisfied: typing-extensions in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (4.12.2)\n","Requirement already satisfied: sympy in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (1.13.1)\n","Requirement already satisfied: networkx in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (3.1)\n","Requirement already satisfied: jinja2 in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /home/user/.local/lib/python3.10/site-packages (from torch>=1.2.0->awesome-align==0.1.7) (2.0.0)\n","Requirement already satisfied: cmake in /home/user/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.2.0->awesome-align==0.1.7) (3.26.3)\n","Requirement already satisfied: lit in /home/user/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.2.0->awesome-align==0.1.7) (16.0.5)\n","Requirement already satisfied: botocore<1.32.0,>=1.31.9 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (1.31.9)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/user/.local/lib/python3.10/site-packages (from boto3->awesome-align==0.1.7) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/.local/lib/python3.10/site-packages (from requests->awesome-align==0.1.7) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->awesome-align==0.1.7) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/.local/lib/python3.10/site-packages (from requests->awesome-align==0.1.7) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->awesome-align==0.1.7) (2020.6.20)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/user/.local/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.9->boto3->awesome-align==0.1.7) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/user/.local/lib/python3.10/site-packages (from jinja2->torch>=1.2.0->awesome-align==0.1.7) (2.1.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/.local/lib/python3.10/site-packages (from sympy->torch>=1.2.0->awesome-align==0.1.7) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.9->boto3->awesome-align==0.1.7) (1.16.0)\n","Installing collected packages: awesome-align\n","  Attempting uninstall: awesome-align\n","    Found existing installation: awesome-align 0.1.7\n","    Uninstalling awesome-align-0.1.7:\n","      Successfully uninstalled awesome-align-0.1.7\n","  Running setup.py develop for awesome-align\n","Successfully installed awesome-align\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Using device: GPU (NVIDIA RTX A4000)\n"]},{"name":"stderr","output_type":"stream","text":["/home/user/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing punjabi...\n"]},{"name":"stderr","output_type":"stream","text":["First Filter: Checking NER: 100%|\n"]},{"name":"stdout","output_type":"stream","text":["Total sentences: 2980383\n","  With entity: 1474208\n","  Without entity (sampled 1%): 15061 of 1506175\n","  => after first filter: 1489269\n","2025-04-29 04:25:05.357496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Loading the dataset...\n","Extracting: 279040it [45:46, 89.01it/s] Traceback (most recent call last):\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/run_align.py\", line 297, in <module>\n","    main()\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/run_align.py\", line 294, in main\n","    word_align(args, model, tokenizer)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/run_align.py\", line 174, in word_align\n","    word_aligns_list = model.get_aligned_word(ids_src, ids_tgt, bpe2word_map_src, bpe2word_map_tgt, args.device, 0, 0, align_layer=args.align_layer, extraction=args.extraction, softmax_threshold=args.softmax_threshold, test=True, output_prob=(args.output_prob_file is not None))\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 691, in get_aligned_word\n","    outputs_tgt = self.bert(\n","  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 535, in forward\n","    encoder_outputs = self.encoder(\n","  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 342, in forward\n","    layer_outputs = layer_module(\n","  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 321, in forward\n","    attention_outputs = self.attention(hidden_states, attention_mask)\n","  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 273, in forward\n","    self_outputs = self.self(\n","  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/home/user/112101050_Utsav/btp/samanantar_align/awesome-align/awesome_align/modeling.py\", line 226, in forward\n","    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 15.73 GiB total capacity; 6.15 GiB already allocated; 2.16 GiB free; 12.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","Extracting: 279040it [45:49, 101.48it/s]\n","2025-04-29 05:11:11.529979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Loading the dataset...\n","Extracting: 1489269it [4:03:59, 101.73it/s]\n","  => after second filter (top 30%): 446780\n"]},{"name":"stderr","output_type":"stream","text":["Final Tag Transfer: NER on Englis\n","Transferring tags:  16%|▏| 69771/Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n","Transferring tags: 100%|█| 446780\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- PUNJABI VERIFICATION SAMPLE ---\n","English Sentence: Polyclinic Chandigarh\n","English Tokens & Tags: [('Pol', 'B-ORG'), ('##y', 'I-ORG'), ('##clin', 'I-ORG'), ('##ic', 'I-ORG'), ('Chan', 'I-ORG'), ('##dig', 'I-ORG'), ('##ar', 'I-ORG'), ('##h', 'I-ORG')]\n","Target Sentence: ਚੰਡੀਗੜ ਹੋਮੀਉਪੈਥਿਕ ਕਲੀਨਿਕ\n","Target Final Tags: [('ਚੰਡੀਗੜ', 'B-ORG'), ('ਹੋਮੀਉਪੈਥਿਕ', 'O'), ('ਕਲੀਨਿਕ', 'O')]\n","--------------------------------------------------\n","Saved 443483 tagged sentences to /home/user/112101050_Utsav/btp/samanantar_align/samanantar_data/new_bio_tagged_samanantar/punjabi_ner.json\n"]}],"source":["#this is the same code as above with a different model which we fine tuned on 400 manually annotated punjabi sentences\n","\n","# below is the terminal command for the fine tuning\n","\n","'''\n","# (already in virtualenv)\n","\n","pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","pip install -r requirements.txt\n","\n","export TRAIN_FILE=/home/user/112101050_Utsav/btp/align_train_hm/enpa.src-tgt\n","export TRAIN_GOLD_FILE=/home/user/112101050_Utsav/btp/align_train_hm/enpa.gold\n","export OUTPUT_DIR=/home/user/112101050_Utsav/btp/align_train_hm/awesome_output\n","\n","CUDA_VISIBLE_DEVICES=0 awesome-train \\\n","    --output_dir=$OUTPUT_DIR \\\n","    --model_name_or_path=bert-base-multilingual-cased \\\n","    --extraction softmax \\\n","    --do_train \\\n","    --train_so \\\n","    --train_data_file=$TRAIN_FILE \\\n","    --train_gold_file=$TRAIN_GOLD_FILE \\\n","    --per_gpu_train_batch_size 2 \\\n","    --gradient_accumulation_steps 4 \\\n","    --num_train_epochs 5 \\\n","    --learning_rate 1e-4 \\\n","    --save_steps 200\n","'''\n","\n","# Install required packages and clone awesome-align\n","!pip install transformers\n","!git clone https://github.com/neulab/awesome-align.git\n","%cd awesome-align\n","!pip install -e .\n","\n","import os\n","import zipfile\n","import json\n","import random\n","import concurrent.futures\n","from pathlib import Path\n","from collections import defaultdict\n","from functools import reduce\n","import operator\n","\n","import torch\n","from tqdm import tqdm\n","from transformers import pipeline, AutoTokenizer\n","\n","# ----- PARAMETERS & PATHS -----\n","DATA_DIR = Path(\"/home/user/112101050_Utsav/btp/samanantar_align/samanantar_data\")\n","DATA_DIR.mkdir(exist_ok=True)\n","ZIP_FILE = DATA_DIR / \"samanantar.zip\"\n","EXTRACT_DIR = DATA_DIR / \"extracted\"\n","\n","# Languages to process\n","LANGUAGE_CODES = {\n","    # \"hindi\": \"hi\",\n","    # \"assamese\": \"as\",\n","    # \"gujarati\": \"gu\",\n","    # \"marathi\": \"mr\",\n","    \"punjabi\": \"pa\"\n","}\n","\n","# Filtering parameters\n","MIN_TOKENS = 1          # minimum number of tokens per sentence\n","MAX_TOKENS = 100        # maximum number of tokens per sentence\n","\n","# ----- DEVICE SETUP -----\n","device_name = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n","print(f\"Using device: {device_name} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n","device = 0 if torch.cuda.is_available() else -1\n","if device != -1:\n","    torch.cuda.set_device(device)\n","\n","# ----- INITIALIZE TOKENIZER -----\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","\n","# ----- INITIALIZE NER PIPELINE -----\n","def create_ner_pipeline(device: int = -1):\n","    return pipeline(\n","        \"ner\",\n","        model=\"dslim/bert-base-NER\",\n","        tokenizer=\"dslim/bert-base-NER\",\n","        device=device,\n","        batch_size=16,\n","        aggregation_strategy=\"none\",\n","        torch_dtype=torch.float16 if device != -1 else None\n","    )\n","ner_pipeline = create_ner_pipeline(device=device)\n","\n","\n","# ----- VALIDATE AND EXTRACT DATASET ZIP -----\n","def validate_and_extract(zip_path, extract_path):\n","    if not zip_path.exists():\n","        raise FileNotFoundError(f\"ZIP file not found at {zip_path}.\")\n","    try:\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            bad_file = zip_ref.testzip()\n","            if bad_file:\n","                raise ValueError(f\"Corrupted file in ZIP: {bad_file}\")\n","    except Exception as e:\n","        raise ValueError(f\"Invalid ZIP file: {e}\") from None\n","\n","    if not extract_path.exists():\n","        extract_path.mkdir(parents=True, exist_ok=True)\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            with concurrent.futures.ThreadPoolExecutor() as executor:\n","                for file in zip_ref.namelist():\n","                    executor.submit(zip_ref.extract, file, extract_path)\n","        print(\"Extraction complete.\")\n","\n","#validate_and_extract(ZIP_FILE, EXTRACT_DIR)\n","\n","# ----- READ SENTENCE FILES -----\n","def process_language_file(en_file, lang_file):\n","    with open(en_file, 'r', encoding='utf-8') as en_f, open(lang_file, 'r', encoding='utf-8') as lang_f:\n","        english_sentences = [line.strip() for line in en_f if line.strip()]\n","        language_sentences = [line.strip() for line in lang_f if line.strip()]\n","    assert len(english_sentences) == len(language_sentences), \"Mismatch in sentence counts.\"\n","    return english_sentences, language_sentences\n","\n","# ----- MERGE ENTITIES -----\n","def merge_entities(entities, sentence):\n","    \"\"\"\n","    Merges consecutive entities of the same type if they are adjacent or overlapping.\n","    \"\"\"\n","    merged = []\n","    for entity in sorted(entities, key=lambda x: x['start']):\n","        full_tag = entity['entity']\n","        ent_type = full_tag.split('-')[-1] if '-' in full_tag else full_tag\n","        start, end = entity['start'], entity['end']\n","        if merged and merged[-1]['ent_type'] == ent_type:\n","            prev_end = merged[-1]['end']\n","            # If there's no actual text between previous entity and this entity, merge them\n","            if not sentence[prev_end:start].strip():\n","                merged[-1]['end'] = max(merged[-1]['end'], end)\n","                continue\n","        merged.append({'start': start, 'end': end, 'ent_type': ent_type})\n","    return merged\n","\n","# ----- STEP 1: FILTER BY ENTITY (FIRST FILTER) -----\n","def filter_by_entity(english_sentences, language_sentences):\n","    \"\"\"\n","    Runs NER on all English sentences to decide which have named entities.\n","    Keep all sentences that contain >=1 entity.\n","    Keep only 1% of those that have no entities.\n","    Returns the filtered subset of (english_sentences, language_sentences).\n","    \"\"\"\n","    assert len(english_sentences) == len(language_sentences)\n","\n","    batch_size = 64\n","    n = len(english_sentences)\n","    has_entity = [False] * n\n","\n","    # Process NER in batches\n","    idx = 0\n","    for start_idx in tqdm(range(0, n, batch_size), desc=\"First Filter: Checking NER\"):\n","        batch = english_sentences[start_idx:start_idx+batch_size]\n","        ner_results = ner_pipeline(batch)\n","        for i, entities in enumerate(ner_results):\n","            merged = merge_entities(entities, batch[i])\n","            if merged:  # if there's at least one entity\n","                has_entity[start_idx + i] = True\n","\n","    entity_indices = [i for i, val in enumerate(has_entity) if val]\n","    non_entity_indices = [i for i, val in enumerate(has_entity) if not val]\n","\n","    # We keep all entity sentences\n","    # For non-entity sentences, keep only 1%\n","    random.shuffle(non_entity_indices)\n","    sample_size = max(1, int(0.01 * len(non_entity_indices))) if len(non_entity_indices) > 0 else 0\n","    sampled_non_entity_indices = set(non_entity_indices[:sample_size])\n","\n","    final_indices = entity_indices + list(sampled_non_entity_indices)\n","    final_indices.sort()\n","\n","    filtered_en = [english_sentences[i] for i in final_indices]\n","    filtered_lang = [language_sentences[i] for i in final_indices]\n","\n","    print(f\"Total sentences: {len(english_sentences)}\")\n","    print(f\"  With entity: {len(entity_indices)}\")\n","    print(f\"  Without entity (sampled 1%): {sample_size} of {len(non_entity_indices)}\")\n","    print(f\"  => after first filter: {len(filtered_en)}\")\n","\n","    return filtered_en, filtered_lang\n","\n","# ----- RUN ALIGNMENT & KEEP TOP 30% (SECOND FILTER) -----\n","def run_alignment(english_sentences, language_sentences):\n","    \"\"\"\n","    Runs awesome-align forward & backward. Computes alignment-based scores\n","    (normalized by the number of words in each pair). Sorts by these scores\n","    and keeps top 30%.\n","    \"\"\"\n","    batch_size = 256 if device != -1 else 128\n","    forward_input = \"\\n\".join([f\"{eng} ||| {lang}\" for eng, lang in zip(english_sentences, language_sentences)])\n","    forward_input_path = f\"{DATA_DIR}/forward_input.txt\"\n","    forward_output_path = f\"{DATA_DIR}/forward_output.txt\"\n","\n","    with open(forward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(forward_input)\n","\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" if device != -1 else \"-1\"\n","\n","    # Forward alignment\n","    !python3 run_align.py \\\n","      --model_name_or_path \"/home/user/112101050_Utsav/btp/align_train_hm/awesome_output\" \\\n","      --data_file \"{forward_input_path}\" \\\n","      --output_file \"{forward_output_path}\" \\\n","      --output_prob_file \"{DATA_DIR}/forward_probabilities.txt\" \\\n","      --output_word_file \"{DATA_DIR}/forward_word_pairs.txt\" \\\n","      --extraction 'softmax' \\\n","      --batch_size {batch_size} \\\n","      --cache_dir \"{DATA_DIR}/cache\"\n","\n","    # Backward alignment\n","    backward_input = \"\\n\".join([f\"{lang} ||| {eng}\" for eng, lang in zip(english_sentences, language_sentences)])\n","    backward_input_path = f\"{DATA_DIR}/backward_input.txt\"\n","    backward_output_path = f\"{DATA_DIR}/backward_output.txt\"\n","\n","    with open(backward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(backward_input)\n","\n","    !python3 run_align.py \\\n","      --model_name_or_path \"/home/user/112101050_Utsav/btp/align_train_hm/awesome_output\" \\\n","      --data_file \"{backward_input_path}\" \\\n","      --output_file \"{backward_output_path}\" \\\n","      --output_prob_file \"{DATA_DIR}/backward_probabilities.txt\" \\\n","      --output_word_file \"{DATA_DIR}/backward_word_pairs.txt\" \\\n","      --extraction 'softmax' \\\n","      --batch_size {batch_size} \\\n","      --cache_dir \"{DATA_DIR}/cache\"\n","\n","    def read_alignments(path, expected_length):\n","        alignments = []\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                pairs = set()\n","                if line.strip():\n","                    for pair in line.strip().split():\n","                        try:\n","                            e, l = map(int, pair.split('-'))\n","                            pairs.add((e, l))\n","                        except:\n","                            continue\n","                alignments.append(pairs)\n","        # Ensure length matches\n","        while len(alignments) < expected_length:\n","            alignments.append(set())\n","        return alignments[:expected_length]\n","\n","    forward_alignments = read_alignments(forward_output_path, len(english_sentences))\n","    backward_alignments = read_alignments(backward_output_path, len(english_sentences))\n","\n","    # Read probabilities, but use \"average\" normalized by total words to match the second filter requirement\n","    def read_probabilities(prob_file, eng_sents, lang_sents):\n","        scores = []\n","        with open(prob_file, 'r', encoding='utf-8') as f:\n","            for i, line in enumerate(f):\n","                numbers = list(map(float, line.strip().split()))\n","                # Normalized by total words in both sentences (or just English, if desired)\n","                total_words = len(eng_sents[i].split()) + len(lang_sents[i].split())\n","                if total_words == 0:\n","                    score = 0\n","                else:\n","                    score = sum(numbers) / float(total_words)\n","                scores.append(score)\n","        # Ensure scores length matches\n","        while len(scores) < len(eng_sents):\n","            scores.append(0)\n","        return scores\n","\n","    forward_scores = read_probabilities(f\"{DATA_DIR}/forward_probabilities.txt\",\n","                                        english_sentences, language_sentences)\n","\n","    # Sort by forward scores, pick top 30%\n","    threshold_index = int(len(forward_scores) * 0.3)\n","    # Indices sorted by forward_scores in descending order\n","    sorted_indices = sorted(range(len(forward_scores)), key=lambda i: forward_scores[i], reverse=True)\n","    top_indices = sorted_indices[:threshold_index]\n","\n","    # Keep all top 30% sentences regardless of alignment intersection\n","    final_english = [english_sentences[i] for i in top_indices]\n","    final_lang = [language_sentences[i] for i in top_indices]\n","    final_alignments = [{\"alignment\": forward_alignments[i] & backward_alignments[i]} for i in top_indices]\n","\n","    print(f\"  => after second filter (top 30%): {len(final_english)}\")\n","\n","    return final_english, final_lang, final_alignments\n","\n","def get_word_spans(sentence):\n","    \"\"\"Extract words and their character spans from a sentence using whitespace tokenization.\"\"\"\n","    words = []\n","    start = 0\n","    for word in sentence.split():\n","        # Find the start index of the word in the sentence\n","        while start < len(sentence) and sentence[start] == ' ':\n","            start += 1\n","        end = start + len(word)\n","        words.append((word, (start, end)))\n","        start = end\n","    return words\n","\n","# ----- TOKENIZATION UTILITY -----\n","def tokenize_with_offsets(sentence, tokenizer):\n","    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n","    offsets = encoding[\"offset_mapping\"]\n","    return tokens, offsets\n","\n","# ----- TRANSFER TAGS (FINAL STEP) -----\n","def transfer_tags(english_sentences, language_sentences, alignments):\n","    curated_tagged_dataset = []\n","    sample_data = {}\n","\n","    ner_results = []\n","    batch_size = 64\n","    for batch_start in tqdm(range(0, len(english_sentences), batch_size),\n","                          desc=\"Final Tag Transfer: NER on English\"):\n","        batch = english_sentences[batch_start:batch_start+batch_size]\n","        ner_results.extend(ner_pipeline(batch))\n","\n","    for idx, (eng_sent, lang_sent, align_info) in tqdm(\n","            enumerate(zip(english_sentences, language_sentences, alignments)),\n","            total=len(english_sentences),\n","            desc=\"Transferring tags\"\n","        ):\n","        # English tokenization and tagging (unchanged)\n","        eng_tokens, eng_offsets = tokenize_with_offsets(eng_sent, bert_tokenizer)\n","        if not (MIN_TOKENS <= len(eng_tokens) <= MAX_TOKENS):\n","            continue\n","\n","        english_entities = merge_entities(ner_results[idx], eng_sent)\n","        bio_tags = ['O'] * len(eng_tokens)\n","        for entity in english_entities:\n","            ent_type = entity['ent_type']\n","            ent_start = entity['start']\n","            ent_end = entity['end']\n","            overlapping_indices = []\n","            for i, (tok_start, tok_end) in enumerate(eng_offsets):\n","                if max(tok_start, ent_start) < min(tok_end, ent_end):\n","                    overlapping_indices.append(i)\n","            if overlapping_indices:\n","                bio_tags[overlapping_indices[0]] = f\"B-{ent_type}\"\n","                for j in overlapping_indices[1:]:\n","                    bio_tags[j] = f\"I-{ent_type}\"\n","\n","        # Target language processing (modified)\n","        lang_subword_tokens, lang_subword_offsets = tokenize_with_offsets(lang_sent, bert_tokenizer)\n","        if len(lang_subword_tokens) == 0:\n","            continue\n","\n","        alignment_map = defaultdict(list)\n","        for e_idx, l_idx in align_info[\"alignment\"]:\n","            if e_idx < len(eng_tokens) and l_idx < len(lang_subword_tokens):\n","                alignment_map[l_idx].append(e_idx)\n","\n","        lang_subword_tags = ['O'] * len(lang_subword_tokens)\n","        for l_idx in range(len(lang_subword_tags)):\n","            for e_idx in alignment_map[l_idx]:\n","                eng_tag = bio_tags[e_idx]\n","                if eng_tag != 'O':\n","                    lang_subword_tags[l_idx] = eng_tag\n","                    break\n","\n","        # Group subwords into words\n","        word_spans = get_word_spans(lang_sent)\n","        word_level_tokens = []\n","        word_level_tags = []\n","\n","        for word, (s_start, s_end) in word_spans:\n","            subword_indices = []\n","            for idx, (tok_start, tok_end) in enumerate(lang_subword_offsets):\n","                if tok_start >= s_start and tok_end <= s_end:\n","                    subword_indices.append(idx)\n","\n","            tags_in_word = [lang_subword_tags[idx] for idx in subword_indices\n","                          if idx < len(lang_subword_tags)]\n","\n","            # Take first non-O tag (now keeps I- tags as-is)\n","            word_tag = next((tag for tag in tags_in_word if tag != 'O'), 'O')\n","            word_level_tokens.append(word)\n","            word_level_tags.append(word_tag)\n","\n","        # Enforce strict BIO constraints\n","        current_ent = None\n","        for i in range(len(word_level_tags)):\n","            tag = word_level_tags[i]\n","\n","            if tag == 'O':\n","                current_ent = None\n","                continue\n","\n","            parts = tag.split('-', 1)\n","            if len(parts) != 2:\n","                word_level_tags[i] = 'O'\n","                continue\n","\n","            prefix, ent_type = parts\n","\n","            # Handle invalid I- at start or after O\n","            if prefix == 'I':\n","                if current_ent != ent_type:\n","                    # Convert to B- if not continuation\n","                    word_level_tags[i] = f'B-{ent_type}'\n","                    current_ent = ent_type\n","                else:\n","                    # Valid continuation, keep I-\n","                    pass\n","            elif prefix == 'B':\n","                current_ent = ent_type\n","            else:\n","                word_level_tags[i] = 'O'\n","                current_ent = None\n","\n","        # Final validation pass\n","        prev_tag = None\n","        for i in range(len(word_level_tags)):\n","            tag = word_level_tags[i]\n","            if tag.startswith('I-'):\n","                if prev_tag in [None, 'O'] or not prev_tag.endswith(tag.split('-')[1]):\n","                    # Convert standalone I- to B-\n","                    word_level_tags[i] = tag.replace('I-', 'B-', 1)\n","            prev_tag = word_level_tags[i]\n","\n","        # Apply length filter\n","        if not (MIN_TOKENS <= len(word_level_tokens) <= MAX_TOKENS):\n","            continue\n","\n","        # Store sample data\n","        if not sample_data:\n","            sample_data = {\n","                'english': eng_sent,\n","                'target': lang_sent,\n","                'english_tokens': list(zip(eng_tokens, bio_tags)),\n","                'target_final': list(zip(word_level_tokens, word_level_tags))\n","            }\n","\n","        curated_tagged_dataset.append(list(zip(word_level_tokens, word_level_tags)))\n","\n","    return curated_tagged_dataset, sample_data\n","\n","# ----- PROCESS ALL LANGUAGES -----\n","def process_all_languages():\n","    for lang, lang_code in LANGUAGE_CODES.items():\n","        lang_dir = EXTRACT_DIR / \"v2\" / f\"en-{lang_code}\"\n","        en_file = lang_dir / \"train.en\"\n","        lang_file = lang_dir / f\"train.{lang_code}\"\n","        if not en_file.exists() or not lang_file.exists():\n","            print(f\"Skipping {lang} - files not found\")\n","            continue\n","\n","        print(f\"\\nProcessing {lang}...\")\n","\n","        # 1) Read all sentences\n","        english_sentences, language_sentences = process_language_file(en_file, lang_file)\n","\n","        # 2) First Filter: Keep all NE + 1% No NE\n","        filtered_english_1, filtered_lang_1 = filter_by_entity(english_sentences, language_sentences)\n","\n","        # 3) Second Filter (Alignment-based): top 30% after scoring\n","        final_english, final_lang, alignments = run_alignment(filtered_english_1, filtered_lang_1)\n","\n","        # 4) Transfer tags\n","        tagged_language_dataset, sample_data = transfer_tags(final_english, final_lang, alignments)\n","\n","        # Show a verification sample\n","        print(f\"\\n--- {lang.upper()} VERIFICATION SAMPLE ---\")\n","        print(\"English Sentence:\", sample_data.get('english', 'N/A'))\n","        print(\"English Tokens & Tags:\", sample_data.get('english_tokens', 'N/A'))\n","        print(\"Target Sentence:\", sample_data.get('target', 'N/A'))\n","        print(\"Target Final Tags:\", sample_data.get('target_final', 'N/A'))\n","        print(\"-\" * 50)\n","\n","        # 5) Save to JSON\n","        json_dataset = []\n","        for tagged_sentence in tagged_language_dataset:\n","            tokens = [token for token, tag in tagged_sentence]\n","            tags = [tag for token, tag in tagged_sentence]\n","            json_dataset.append({\"tokens\": tokens, \"ner_tags\": tags})\n","\n","        output_dir = DATA_DIR / \"new_bio_tagged_samanantar\"\n","        output_dir.mkdir(exist_ok=True)\n","        output_file = output_dir / f\"{lang}_ner.json\"\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            json.dump(json_dataset, f, ensure_ascii=False, indent=2)\n","        print(f\"Saved {len(json_dataset)} tagged sentences to {output_file}\")\n","\n","\n","process_all_languages()\n","\n"]},{"cell_type":"raw","metadata":{"id":"IV1ELqNSK8nM"},"source":["##getting the gold files\n","\n","import os\n","import json\n","import random\n","import zipfile\n","import concurrent.futures\n","from pathlib import Path\n","from collections import defaultdict\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import pipeline, AutoTokenizer\n","\n","# ----- PARAMETERS & PATHS -----\n","DATA_DIR = Path(\"/home/user/112101050_Utsav/btp/samanantar_align/samanantar_data\")\n","EXTRACT_DIR = DATA_DIR / \"extracted\"  # expects files under EXTRACT_DIR/v2/en-pa\n","OUTPUT_DIR = DATA_DIR / \"punjabi_alignment_output\"\n","OUTPUT_DIR.mkdir(exist_ok=True)\n","\n","# We are processing Punjabi only\n","LANG_CODE = \"pa\"\n","LANG_DIR = EXTRACT_DIR / \"v2\" / f\"en-{LANG_CODE}\"\n","\n","# Files from Samanantar\n","EN_FILE = LANG_DIR / \"train.en\"\n","TARGET_FILE = LANG_DIR / f\"train.{LANG_CODE}\"\n","\n","# Filtering parameters\n","MIN_TOKENS = 1\n","MAX_TOKENS = 100\n","SAMPLE_COUNT = 300  # Number of sentence pairs to pick\n","\n","# ----- DEVICE SETUP -----\n","device = 0 if torch.cuda.is_available() else -1\n","if device != -1:\n","    torch.cuda.set_device(device)\n","print(f\"Using device: {'GPU' if device != -1 else 'CPU'}\")\n","\n","# ----- INITIALIZE TOKENIZER & NER PIPELINE -----\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","def create_ner_pipeline(device: int = -1):\n","    return pipeline(\n","        \"ner\",\n","        model=\"dslim/bert-base-NER\",\n","        tokenizer=\"dslim/bert-base-NER\",\n","        device=device,\n","        batch_size=16,\n","        aggregation_strategy=\"none\",\n","        torch_dtype=torch.float16 if device != -1 else None\n","    )\n","\n","ner_pipeline = create_ner_pipeline(device=device)\n","\n","# ----- DATA READING -----\n","def process_language_file(en_file, target_file):\n","    with open(en_file, 'r', encoding='utf-8') as en_f, open(target_file, 'r', encoding='utf-8') as tgt_f:\n","        english_sentences = [line.strip() for line in en_f if line.strip()]\n","        target_sentences = [line.strip() for line in tgt_f if line.strip()]\n","    assert len(english_sentences) == len(target_sentences), \"Mismatch in sentence counts.\"\n","    return english_sentences, target_sentences\n","\n","# ----- ENTITY MERGING (for NER results) -----\n","def merge_entities(entities, sentence):\n","    merged = []\n","    for entity in sorted(entities, key=lambda x: x['start']):\n","        full_tag = entity['entity']\n","        ent_type = full_tag.split('-')[-1] if '-' in full_tag else full_tag\n","        start, end = entity['start'], entity['end']\n","        if merged and merged[-1]['ent_type'] == ent_type:\n","            prev_end = merged[-1]['end']\n","            if not sentence[prev_end:start].strip():\n","                merged[-1]['end'] = max(merged[-1]['end'], end)\n","                continue\n","        merged.append({'start': start, 'end': end, 'ent_type': ent_type})\n","    return merged\n","\n","# ----- FILTERING FUNCTION (First filter) -----\n","def filter_by_entity(english_sentences, target_sentences):\n","    batch_size = 64\n","    n = len(english_sentences)\n","    has_entity = [False] * n\n","\n","    for start_idx in tqdm(range(0, n, batch_size), desc=\"Filtering (NER check)\"):\n","        batch = english_sentences[start_idx:start_idx+batch_size]\n","        ner_results = ner_pipeline(batch)\n","        for i, entities in enumerate(ner_results):\n","            merged = merge_entities(entities, batch[i])\n","            if merged:\n","                has_entity[start_idx + i] = True\n","\n","    entity_indices = [i for i, val in enumerate(has_entity) if val]\n","    non_entity_indices = [i for i, val in enumerate(has_entity) if not val]\n","\n","    # Keep all sentences with entities; for non-entity ones, keep 1% (at least one if exists)\n","    random.shuffle(non_entity_indices)\n","    sample_size = max(1, int(0.01 * len(non_entity_indices))) if non_entity_indices else 0\n","    sampled_non_entity = set(non_entity_indices[:sample_size])\n","\n","    final_indices = sorted(entity_indices + list(sampled_non_entity))\n","    filtered_en = [english_sentences[i] for i in final_indices]\n","    filtered_tgt = [target_sentences[i] for i in final_indices]\n","    print(f\"Total sentences: {n}\")\n","    print(f\"Sentences with entity: {len(entity_indices)}\")\n","    print(f\"Sentences without entity (sampled 1%): {sample_size}\")\n","    print(f\"After first filter: {len(filtered_en)}\")\n","    return filtered_en, filtered_tgt\n","\n","# ----- ALIGNMENT FUNCTIONS -----\n","def run_alignment(english_sentences, target_sentences):\n","    # Prepare input files\n","    batch_size = 256 if device != -1 else 128\n","\n","    forward_input = \"\\n\".join([f\"{eng} ||| {tgt}\" for eng, tgt in zip(english_sentences, target_sentences)])\n","    forward_input_path = DATA_DIR / \"forward_input.txt\"\n","    forward_output_path = DATA_DIR / \"forward_output.txt\"\n","    with open(forward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(forward_input)\n","\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" if device != -1 else \"-1\"\n","\n","    # Run forward alignment via awesome-align\n","    cmd_forward = f\"python3 run_align.py --model_name_or_path bert-base-multilingual-cased --data_file {forward_input_path} --output_file {forward_output_path} --output_prob_file {DATA_DIR / 'forward_probabilities.txt'} --output_word_file {DATA_DIR / 'forward_word_pairs.txt'} --extraction 'softmax' --batch_size {batch_size} --cache_dir {DATA_DIR / 'cache'}\"\n","    print(\"Running forward alignment...\")\n","    os.system(cmd_forward)\n","\n","    # Run backward alignment: swap order of sentences.\n","    backward_input = \"\\n\".join([f\"{tgt} ||| {eng}\" for eng, tgt in zip(english_sentences, target_sentences)])\n","    backward_input_path = DATA_DIR / \"backward_input.txt\"\n","    backward_output_path = DATA_DIR / \"backward_output.txt\"\n","    with open(backward_input_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(backward_input)\n","\n","    cmd_backward = f\"python3 run_align.py --model_name_or_path bert-base-multilingual-cased --data_file {backward_input_path} --output_file {backward_output_path} --output_prob_file {DATA_DIR / 'backward_probabilities.txt'} --output_word_file {DATA_DIR / 'backward_word_pairs.txt'} --extraction 'softmax' --batch_size {batch_size} --cache_dir {DATA_DIR / 'cache'}\"\n","    print(\"Running backward alignment...\")\n","    os.system(cmd_backward)\n","\n","    def read_alignments(path, expected_length):\n","        alignments = []\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                pairs = set()\n","                if line.strip():\n","                    for pair in line.strip().split():\n","                        try:\n","                            e, t = map(int, pair.split('-'))\n","                            pairs.add((e, t))\n","                        except Exception as e:\n","                            continue\n","                alignments.append(pairs)\n","        while len(alignments) < expected_length:\n","            alignments.append(set())\n","        return alignments[:expected_length]\n","\n","    forward_alignments = read_alignments(forward_output_path, len(english_sentences))\n","    backward_alignments = read_alignments(backward_output_path, len(english_sentences))\n","\n","    final_alignments = []\n","    for fa, ba in zip(forward_alignments, backward_alignments):\n","        inter = fa & ba\n","        if inter:\n","            final_alignments.append(inter)\n","        else:\n","            # Fallback: use union of both outputs if available\n","            union = fa.union(ba)\n","            final_alignments.append(union)\n","    return final_alignments\n","\n","# ----- MAIN PROCESSING PIPELINE FOR PUNJABI -----\n","def main():\n","    # 1. Read Samanantar sentences for Punjabi\n","    english_sentences, punjabi_sentences = process_language_file(EN_FILE, TARGET_FILE)\n","    print(f\"Loaded {len(english_sentences)} sentence pairs.\")\n","\n","    # 2. Filter by entity\n","    filtered_en, filtered_tgt = filter_by_entity(english_sentences, punjabi_sentences)\n","\n","    # 3. Run alignment on filtered sentences\n","    print(\"Running alignment (forward/backward)...\")\n","    alignments = run_alignment(filtered_en, filtered_tgt)\n","\n","    # 4. Sample exactly SAMPLE_COUNT (300) pairs (or take all if fewer)\n","    total_filtered = len(filtered_en)\n","    sample_indices = list(range(total_filtered)) if total_filtered <= SAMPLE_COUNT else random.sample(range(total_filtered), SAMPLE_COUNT)\n","    sample_indices.sort()  # optional sorting\n","\n","    sampled_en = [filtered_en[i] for i in sample_indices]\n","    sampled_tgt = [filtered_tgt[i] for i in sample_indices]\n","    sampled_alignments = [alignments[i] for i in sample_indices]\n","\n","    print(f\"Selected {len(sampled_en)} sentence pairs for output.\")\n","\n","    # 5. Write output files in the fine-tuning format.\n","    src_tgt_file = OUTPUT_DIR / \"punjabi.src-tgt\"\n","    gold_file = OUTPUT_DIR / \"punjabi.gold\"\n","\n","    with open(src_tgt_file, 'w', encoding='utf-8') as f_src:\n","        for eng, tgt in zip(sampled_en, sampled_tgt):\n","            # Delimiter: three spaces, |||, three spaces\n","            f_src.write(f\"{eng}   |||   {tgt}\\n\")\n","\n","    with open(gold_file, 'w', encoding='utf-8') as f_gold:\n","        for eng, tgt, alignment in zip(sampled_en, sampled_tgt, sampled_alignments):\n","            if alignment:\n","                alignment_pairs = [f\"{e+1}-{t+1}\" for (e, t) in sorted(list(alignment))]\n","                line = \" \".join(alignment_pairs)\n","            else:\n","                # Fallback: tokenize both sentences and align word-by-word\n","                eng_tokens = bert_tokenizer.tokenize(eng)\n","                tgt_tokens = bert_tokenizer.tokenize(tgt)\n","                min_len = min(len(eng_tokens), len(tgt_tokens))\n","                fallback_pairs = [f\"{i+1}-{i+1}\" for i in range(min_len)]\n","                line = \" \".join(fallback_pairs)\n","            f_gold.write(line + \"\\n\")\n","\n","    print(f\"Saved source-target pairs to: {src_tgt_file}\")\n","    print(f\"Saved gold alignments to: {gold_file}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}